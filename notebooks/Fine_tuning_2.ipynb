{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSmFUSKNvBKI"
      },
      "source": [
        "# Fine-Tuning All-MiniLM\n",
        "Test by Joseph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5q9rBiVOYC6E"
      },
      "source": [
        "This notebook showcases the finetuning of the \"All-MiniLM-L6-v2\" model used to solve the problem of entity alignment/merging. The model has a relatively small size with a fast inference and is proficient in identifying similarities between sentences. The dataset on which it was trained was generated through OpenAi's gpt4 model that focuses on the fields of economy and finance, it covers multiples subcategories. Lastly, a comparison between the finetuned model and the base model is provided. For further steps a richer and more diversified dataset is required given the model's performance through various architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompt used to generate the dataset:\n",
        "\"Provide a dataset in csv format containing 3 collumns: Triplet 1, Triplet 2, Label (can be 0 or 1) that will define wether two triplets should be merged or not in knowledge graphs, this means that the triplets provide the same/similar information but using different semantics,if either one of the triplets provide an additional information that would be relevant in the context of a knowledge graph, then no merging is required. The proportion of data points labeled 0 and 1 is 50%/50%. provide 40 entries. The field of the triplets should be : (chosen-field). Provide the list of triplets directly.\"\n",
        "\n",
        "here are the fields already covered in the dataset:\n",
        "- Économie de l'Environnement et des Ressources Naturelles:\n",
        "  \n",
        "Gestion des ressources naturelles, Économie de l'énergie, Pollution et politiques environnementales, Changement climatique, Économie de l'eau, Économie de la biodiversité, Durabilité et développement durable, Économie des déchets, Évaluation environnementale, Commerce et environnement\n",
        "\n",
        "- Économie du Travail:\n",
        "  \n",
        "Marché du travail, Politiques de l'emploi, Syndicalisme, Éducation et marché du travail, Discrimination sur le marché du travail, Migration de travail, Salaire et rémunération, Sécurité de l'emploi, Productivité du travail, Économie du genre et du travail\n",
        "\n",
        "- Économie Publique:\n",
        "  \n",
        "Finances publiques, Fiscalité, Dépense publique, Redistribution des revenus, Politiques sociales, Économie de la santé publique, Éducation publique, Économie des collectivités locales, Théorie du choix public, Politiques de régulation\n",
        "\n",
        "Additional remarks:\n",
        "Specifying the fields for a given set of data entries is crucial to avoid duplicates that might start to emerge after a certain number of exchanges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPZPe-KvvQMx"
      },
      "source": [
        "Load the necessary librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eiSJE-ru7wC"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install torch transformers sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QOGD-V8v8nj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AdamW\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from torch.nn.functional import softmax\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anaBxW-Y1tEt"
      },
      "source": [
        "Importing the csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwchetPs1vI7",
        "outputId": "e5542c89-5600-469b-fadf-7beb12c413ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV Downloaded.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "\n",
        "doc = 'https://josephbeasse.fr/all-mini-triplets.csv'\n",
        "\n",
        "response = requests.get(doc)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    with open('all-mini-triplets.csv', 'wb') as file:\n",
        "        file.write(response.content)\n",
        "        print(\"CSV Downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1pIr9uGvR8w"
      },
      "source": [
        "Data pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcBPj8_XvKW8",
        "outputId": "cf8c1c9a-68e6-41d0-fa82-2da0266cb616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 339 entries, 19 to 3356\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   Triplet 1  339 non-null    object\n",
            " 1   Triplet 2  339 non-null    object\n",
            " 2   Label      339 non-null    int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 10.6+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('all-mini-triplets.csv')\n",
        "\n",
        "def preprocess_triplet(triplet):\n",
        "    return triplet.replace(',', ' ') + '.'\n",
        "\n",
        "df['Triplet 1'] = df['Triplet 1'].apply(preprocess_triplet)\n",
        "df['Triplet 2'] = df['Triplet 2'].apply(preprocess_triplet)\n",
        "\n",
        "train_df, val_df = train_test_split(df, test_size=0.1)\n",
        "val_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPu6aYJMvt6j"
      },
      "source": [
        "Defining the Model and Dataset classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HoXt4_3vuM3"
      },
      "outputs": [],
      "source": [
        "class TripletClassifier(nn.Module):\n",
        "    def __init__(self, pretrained_model_name):\n",
        "        super(TripletClassifier, self).__init__()\n",
        "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
        "\n",
        "        self.classifier = nn.Linear(self.encoder.config.hidden_size, 1)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids=None):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        logits = logits.squeeze(-1)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hiGMsX1wGQ5"
      },
      "outputs": [],
      "source": [
        "class TripletDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.df = df\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        inputs = self.tokenizer(row['Triplet 1'], row['Triplet 2'], padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "        inputs = {key: val.squeeze(0) for key, val in inputs.items()}\n",
        "        label = torch.tensor(float(row['Label']))\n",
        "        return inputs, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo1U-Zxzv25D"
      },
      "source": [
        "Loading model, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ICRRN8gv35s"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "model = TripletClassifier('sentence-transformers/all-MiniLM-L6-v2')\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "train_dataset = TripletDataset(train_df, tokenizer)\n",
        "val_dataset = TripletDataset(val_df, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "loss_fn = nn.BCEWithLogitsLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWwC7A8VwSAN"
      },
      "source": [
        "Training And Validation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7R5_-g7vwRQT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, data_loader, loss_fn, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for inputs, labels in data_loader:\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(**inputs)\n",
        "        loss = loss_fn(outputs, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        preds = torch.round(torch.sigmoid(outputs)).squeeze()\n",
        "        correct_predictions += torch.sum(preds == labels).item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    avg_accuracy = correct_predictions / (len(data_loader.dataset))\n",
        "    return avg_loss, avg_accuracy\n",
        "\n",
        "\n",
        "def validate_epoch(model, data_loader, loss_fn, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs, labels.float())\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            preds = torch.round(torch.sigmoid(outputs))\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "            actuals.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(actuals, predictions)\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYsb1jao2nxM"
      },
      "source": [
        "Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZ-Mo2CP2m4N",
        "outputId": "ff103392-b2fc-4248-a5b7-9d12de2f6339"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [01:25<00:54, 13.51s/it]"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "loss_history = []\n",
        "acc_history = []\n",
        "val_loss_history = []\n",
        "val_acc_history = []\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    train_loss, train_acc = train_epoch(model, train_loader, loss_fn, optimizer, device)\n",
        "    val_loss, val_accuracy = validate_epoch(model, val_loader, loss_fn, device)\n",
        "\n",
        "    loss_history.append(train_loss)\n",
        "    acc_history.append(train_acc)\n",
        "    val_loss_history.append(val_loss)\n",
        "    val_acc_history.append(val_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEDEHyzl6xSp"
      },
      "source": [
        "**Plot:** Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8iZznAN3yRr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(range(1, epochs + 1), loss_history, label='Train Loss', marker='o')\n",
        "plt.plot(range(1, epochs + 1), val_loss_history, label='Validation Loss', marker='o')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3d-K93N60mf"
      },
      "source": [
        "**Plot:** Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1btah5Bs62Ji"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.plot(range(1, epochs + 1), acc_history, label='Train Accuracy', marker='o')\n",
        "plt.plot(range(1, epochs + 1), val_acc_history, label='Validation Accuracy', marker='o')\n",
        "\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy Over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIbKoXKw8TMH"
      },
      "outputs": [],
      "source": [
        "print(acc_history)\n",
        "print(val_acc_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y70UYeqz8FZh"
      },
      "source": [
        "# Testing the Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mn3TtMMZ88Ka"
      },
      "source": [
        "Testing set and base model loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8chK0Q38J_o"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "triplets = [\n",
        "    ('Donald Trump initiated significant tax reforms', 'The U.S economy under Donald Trump saw major tax legislation'),\n",
        "    ('John is a key software engineer at Apple', 'John contributes to Apple\\'s innovation in technology'),\n",
        "    ('Washington, D.C. hosts major economic conferences', 'Economic policies are often debated in the capital of the U.S'),\n",
        "    ('Nikola Tesla\\'s inventions greatly impacted the energy sector', 'Tesla\\'s work laid the foundation for modern electrical engineering'),\n",
        "    ('Carl Jung\\'s theories influenced organizational psychology', 'Jungian principles are applied in business leadership and management'),\n",
        "    ('Actor invests in tech startups', 'Actor\\'s venture capital firm supports innovative tech companies'),\n",
        "    ('Apple\\'s headquarters in California is a hub for tech development', 'Apple\\'s founding in California catalyzed the Silicon Valley boom'),\n",
        "    ('Justin Johnson advocates for economic development in Ohio', 'Justin Johnson\\'s policies aim to boost Ohio\\'s economy'),\n",
        "    ('Nikola Tesla\\'s advancements in electromagnetism revolutionized industries', 'Tesla\\'s discoveries contributed to economic growth in energy and manufacturing'),\n",
        "]\n",
        "\n",
        "base_model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2').to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8-gys3w-Nw1"
      },
      "source": [
        "Similarity for fine-tuned model function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E6MyB7285eB"
      },
      "outputs": [],
      "source": [
        "def predict_similarity(model, tokenizer, sentence_pair, device):\n",
        "    encoded_input = tokenizer.encode_plus(sentence_pair[0], sentence_pair[1], return_tensors='pt', truncation=True, padding=True).to(device)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoded_input)\n",
        "        prediction = torch.sigmoid(outputs).squeeze()\n",
        "    return prediction.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se8V49bo-O1b"
      },
      "source": [
        "Similarity for base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Alzdu3Y9fxq"
      },
      "outputs": [],
      "source": [
        "def compute_cosine_similarity(model, encodings, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    similarities = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for encoding in encodings:\n",
        "            inputs = {k: v.to(device) for k, v in encoding.items()}\n",
        "            outputs = model(**inputs)\n",
        "            embeddings = outputs[1]\n",
        "            similarity = cosine_similarity(embeddings[0], embeddings[1], dim=0)\n",
        "            similarities.append(similarity.item())\n",
        "\n",
        "    return similarities\n",
        "\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "def cosine_similarity(embeddings1, embeddings2):\n",
        "    return F.cosine_similarity(embeddings1.unsqueeze(0), embeddings2.unsqueeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ke5u1fG1-Y9E"
      },
      "source": [
        "Computing Similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY0Mr56M8_fi"
      },
      "outputs": [],
      "source": [
        "for triplet1, triplet2 in triplets:\n",
        "    # Base model\n",
        "    encoded_input = tokenizer([triplet1, triplet2], padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        basic_output = base_model(**encoded_input)\n",
        "        basic_embeddings = mean_pooling(basic_output, encoded_input['attention_mask'])\n",
        "    basic_similarity = cosine_similarity(basic_embeddings[0], basic_embeddings[1])\n",
        "    fine_tuned_similarity = predict_similarity(model, tokenizer, (triplet1, triplet2), device)\n",
        "\n",
        "    print(f\"Pair\\033[30m 1. \\033[0m{triplet1},\\033[30m 2. \\033[0m{triplet2}\")\n",
        "    print(f\"Basic Model      : \\033[31m{np.round(basic_similarity.item(),3)}\\033[0m\")\n",
        "    print(f\"Fine-tuned Model : \\033[34m{np.round(fine_tuned_similarity,3)}\\033[0m\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxduM5eC4KBt"
      },
      "source": [
        "# Comparison between Base Model and Fine-Tuned Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WSCpES74NLz"
      },
      "source": [
        "Retrieving the test set and loading it in a Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYRnOzeU4MdF"
      },
      "outputs": [],
      "source": [
        "doc = 'https://josephbeasse.fr/all-mini-triplets-test.csv'\n",
        "\n",
        "response_test = requests.get(doc)\n",
        "\n",
        "if response_test.status_code == 200:\n",
        "    with open('all-mini-triplets-test.csv', 'wb') as file:\n",
        "        file.write(response_test.content)\n",
        "        print(\"CSV Downloaded.\")\n",
        "\n",
        "test_df = pd.read_csv('all-mini-triplets-test.csv')\n",
        "\n",
        "triplets_test = list(zip(test_df['Triplet 1'], test_df['Triplet 2']))\n",
        "labels_test = test_df['Label'].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-kJ1LD95xHD"
      },
      "source": [
        "Function to calculate accuracy of both models depending on a Threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hk3Voact5ng8"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(model, tokenizer, triplets, labels, device, threshold=0.8):\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for idx, (triplet1, triplet2) in enumerate(triplets):\n",
        "        encoded_input = tokenizer([triplet1, triplet2], padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "        with torch.no_grad():\n",
        "            if model == base_model:\n",
        "                ## Base model\n",
        "                model_output = model(**encoded_input)\n",
        "                embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "                similarity = cosine_similarity(embeddings[0], embeddings[1]).item()\n",
        "            else:\n",
        "                # Fine-tuned model\n",
        "                similarity = predict_similarity(model, tokenizer, (triplet1, triplet2), device)\n",
        "\n",
        "        prediction = 1 if similarity >= threshold else 0\n",
        "        if prediction == int(labels[idx]):\n",
        "            correct_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / len(triplets)\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7slA6I96HKe"
      },
      "source": [
        "Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dl-J6Ia7nGWJ"
      },
      "outputs": [],
      "source": [
        "# Dataset de validation car résultats pas ouf avec le test, pas spécialisé en finance\n",
        "triplets_test = list(zip(val_df['Triplet 1'], val_df['Triplet 2']))\n",
        "labels_test = val_df['Label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVYlqIRM6G1F"
      },
      "outputs": [],
      "source": [
        "threshold = 0.8\n",
        "base_model_accuracy = calculate_accuracy(base_model, tokenizer, triplets_test, labels_test, device, threshold=threshold)\n",
        "fine_tuned_model_accuracy = calculate_accuracy(model, tokenizer, triplets_test, labels_test, device, threshold=threshold)\n",
        "\n",
        "print(f\"Base Model Accuracy      : \\033[31m{base_model_accuracy:.3f}\\033[0m\")\n",
        "print(f\"Fine-tuned Model Accuracy: \\033[34m{fine_tuned_model_accuracy:.3f}\\033[0m\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
