{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Entity Alignment: Hash Approach"
      ],
      "metadata": {
        "id": "4y14QwN6VdpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook presents a method for entity alignment and merging using sentence embeddings and hashing as an attempt for a more efficient processing. This deals with the problem of merging/entity alignment for our knowledge graph. The notebook includes a hashing function to group similar entity types. Through practical examples, it illustrates the process of merging entities with different similarities, and generates 'also known as' relations for entities with different names but identified as similar. This approach should be further revised."
      ],
      "metadata": {
        "id": "i2VYsQoOWHOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JixDwUFU7MO4",
        "outputId": "d27f95b1-bbfa-4086-86e5-9328f4664822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m873.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence-transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.19.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=94a331d8c1900b87b75e6552bc84c695f61d962fcee47c97c1ce8b087abe5680\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.2.2 sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xg3sgsfJ759l",
        "outputId": "189c9119-cc67-4b32-f272-ba3d8f911fc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "\n",
        "# Initialize the model\n",
        "merge_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "r1 = {'head': 'Université Lyon 2', 'head_type': 'org', 'type': 'subsidiary', 'tail': 'Paris School of Economics', 'tail_type': 'org'}\n",
        "r2 = {'head': 'Université Lumière Lyon 2', 'head_type': 'org', 'type': 'part of', 'tail': 'GATE', 'tail_type': 'org'}\n",
        "\n",
        "# r1 = {'head': 'Napoleon', 'head_type': 'Person', 'type': 'Lead', 'tail': 'War of waterloo', 'tail_type': 'Event'}\n",
        "# r2 = {'head': 'Napoleon Bonaparte', 'head_type': 'Person', 'type': 'Born', 'tail': 'Corsica', 'tail_type': 'Location'}\n",
        "\n",
        "relations = [r1, r2]\n",
        "\n",
        "def encode_relation(relation):\n",
        "    relation_str = ' '.join([relation['head'], relation['type']])\n",
        "    return merge_model.encode(relation_str)\n",
        "\n",
        "def similarity_score(enc1, enc2):\n",
        "    return util.pytorch_cos_sim(enc1, enc2).item()\n",
        "\n",
        "def should_merge(rel1, rel2, threshold):\n",
        "    enc1 = encode_relation(rel1)\n",
        "    enc2 = encode_relation(rel2)\n",
        "    sim = similarity_score(enc1, enc2)\n",
        "    print('Similarity between', rel1['head'], 'and', rel2['head'], 'is :', sim)\n",
        "    return sim >= threshold\n",
        "\n",
        "def hash_relation(relation):\n",
        "    return hashlib.md5(relation['head_type'].encode()).hexdigest()\n",
        "\n",
        "def align_and_merge_relations(relations):\n",
        "    hashed_groups = defaultdict(list)\n",
        "    merged_relations = []\n",
        "    aka_relations = []\n",
        "\n",
        "    for relation in relations:\n",
        "        hash_key = hash_relation(relation)\n",
        "        hashed_groups[hash_key].append(relation)\n",
        "\n",
        "    for group in hashed_groups.values():\n",
        "        merged_relations.extend(merge_group(group, aka_relations))\n",
        "\n",
        "    return merged_relations, aka_relations\n",
        "\n",
        "def merge_group(group, aka_relations, threshold=0.8):\n",
        "    merged = []\n",
        "\n",
        "    while group:\n",
        "        base = group.pop()\n",
        "        merge_candidates = [base]\n",
        "\n",
        "        for other in list(group):\n",
        "            if should_merge(base, other, threshold):\n",
        "                merge_candidates.append(other)\n",
        "                group.remove(other)\n",
        "                # 'also known as' relation if names are different\n",
        "                if base['head'] != other['head']:\n",
        "                    aka_relations.append({'head': base['head'], 'type': 'also known as', 'tail': other['head']})\n",
        "\n",
        "        merged_head = merge_candidates[0]['head']\n",
        "        merged_type = merge_candidates[0]['type']\n",
        "        tails = {cand['tail'] for cand in merge_candidates}\n",
        "        merged_tail = ', '.join(tails)\n",
        "\n",
        "        merged.append({'head': merged_head, 'head_type': 'org', 'type': merged_type, 'tail': merged_tail, 'tail_type': 'org'})\n",
        "\n",
        "    return merged\n",
        "\n",
        "merged_relations, aka_relations = align_and_merge_relations(relations)\n",
        "print(\"Merged Relations:\", merged_relations)\n",
        "print(\"Also Known As Relations:\", aka_relations)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KNGzovU7aLb",
        "outputId": "511a2a98-bc83-49e1-9149-d454cf0f0bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity between Université Lumière Lyon 2 and Université Lyon 2 is : 0.8301360607147217\n",
            "Merged Relations: [{'head': 'Université Lumière Lyon 2', 'head_type': 'org', 'type': 'part of', 'tail': 'Paris School of Economics, GATE', 'tail_type': 'org'}]\n",
            "Also Known As Relations: [{'head': 'Université Lumière Lyon 2', 'type': 'also known as', 'tail': 'Université Lyon 2'}]\n"
          ]
        }
      ]
    }
  ]
}